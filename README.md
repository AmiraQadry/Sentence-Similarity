# Sentence Similarity Using BERT
![](https://miro.medium.com/v2/resize:fit:1280/1*WhtDBvnmtYaujDRCZOIx4w.jpeg)
This project demonstrates how to use BERT (Bidirectional Encoder Representations from Transformers) to compute the similarity between sentences. 
The dataset used for this project is the SST-2 (Stanford Sentiment Treebank), and the similarity is computed using cosine similarity on the embeddings generated by BERT.

## Table of Contents
- [Introduction](#introduction)
- [Dataset](#dataset)
- [Requirements](#requirements)
- [Preprocessing](#preprocessing)
- [Model](#model)
- [Embedding Generation](#embedding-generation)
- [Cosine Similarity](#cosine-similarity)
- [Usage](#usage)
- [Results](#results)
- [Acknowledgments](#acknowledgments)

## Introduction

Sentence similarity is a fundamental task in natural language processing (NLP). It has applications in various domains such as information retrieval, plagiarism detection, and paraphrase identification. 
In this project, we use BERT to generate sentence embeddings and then compute the cosine similarity between these embeddings.

## Dataset

The dataset used in this project is [SST-2](https://www.kaggle.com/datasets/thedevastator/nli-dataset-for-sentence-understanding/data?select=sst2_train.csv) (Stanford Sentiment Treebank 2). 
It contains sentences along with their sentiment labels. We use the sentences to compute their embeddings and then find the similarity between them.

## Requirements

- Python 
- pandas
- nltk
- transformers
- torch
- scikit-learn
- tqdm

You can install the required packages using pip:

```bash
pip install pandas nltk transformers torch scikit-learn tqdm
```

## Preprocessing

The preprocessing step includes tokenizing the sentences, converting them to lowercase, removing non-alphanumeric characters, and removing stopwords. We use NLTK for tokenization and stopword removal.

## Model

We use the pre-trained BERT model (`bert-base-uncased`) from the Transformers library by Hugging Face.

## Embedding Generation

To generate embeddings, we tokenize the sentences and pass them through the BERT model. We use the embeddings of the `[CLS]` token for each sentence.

## Cosine Similarity

We compute the cosine similarity between the embeddings of each pair of sentences.

## Results

The similarity score between two example sentences is computed and displayed. In the provided example, the similarity score is approximately 0.991, indicating high similarity between the two sentences.

## Acknowledgments

- The dataset used is [SST-2](https://www.kaggle.com/datasets/thedevastator/nli-dataset-for-sentence-understanding/data?select=sst2_train.csv) (Stanford Sentiment Treebank 2).
- The BERT model is from the Transformers library by Hugging Face.
- This project uses the PyTorch framework for model inference.

## License

This project is licensed under the MIT License.
